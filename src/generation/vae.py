# ************    VAE for generating synthetic populations      ************
# In this script we define and train a model that takes as input a dataframe of real houses, with geographic coordinates and a set of numeric and categorical features
# and learn an encoder and decoder to reproduce a similar population.
# 
# 
# The class VariationalAutoencoder is composed of the two objects, VariationalEncoder and VariationalDecoder.
# The data in full_df are encoded in the latent space, by applying a NN with the layers defined by hidden_dims.
# In order to simplify the generation of realistic geographical coordinates, we transform the real coordinates into a uniform latent space.
# In this way, the VAE need only to learn the mapping from the latent geographical coordinates, while all the spatial constaints are taken into 
# account by the coordinates transformation. 
# The transformation employed is a Normalizing Flows (defined in nf_utils.py).
# 
# The loss of the VAE is composed by two elements: (i) the reconstruction loss and (ii) the KL-divergence of the encoded data and a Normal distribution 
# (i) measures the distance between the samples generated by the VAE, so it measures how realistic are the samples
# (ii) measures the distance between the encoded space and a Normal distribution, and it controls the overfitting
# In our scenario, we allow for overfitting, as we want to generate samples that can be very close to the collected data.
# (For instance, in image generation, it is typically desired to generate samples different from the collected data)
# Moreover, as the generation of realistic spatial coordinates is the more challenging step, we also allow for stronger overfitting in the geo coordinates.
# For this reason, we set the loss in order to give more weight to reconstruction loss for the geo coordinates, then to reconstruction loss in general, and lower weight to KL-div
# 
# See https://avandekleut.github.io/vae/ as reference
# 
# 
# 
# 





import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils
import torch.distributions
from tqdm import tqdm
from torch.utils.data import DataLoader
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import pandas as pd
import numpy as np
from tqdm import tqdm
import sys
sys.path += [".."]
sys.path += ["../src/evaluation"]
import nf_utils as nfg
from time import time
import pickle
import geopandas as gpd
import utils
import config
from glob import glob



class VariationalEncoder(nn.Module):
    def __init__(self, hidden_dims = []):
        super(VariationalEncoder, self).__init__()
        self.hidden_dims = hidden_dims
        self.linears = nn.ModuleList()
        
        for k in range(len(self.hidden_dims) - 1):
            self.linears.append(nn.Linear(self.hidden_dims[k], self.hidden_dims[k + 1]))
        #add one dims for sigma
        self.linears.append(nn.Linear(self.hidden_dims[k], self.hidden_dims[k + 1]))
        

        self.N = torch.distributions.Normal(0, 1)
        self.kl = 0

    def forward(self, x):
        for linear in self.linears[:-2]:
            # x = F.tanh(linear(x))
            x = F.relu(linear(x))
        mu =  self.linears[-2](x)
        sigma = torch.exp(self.linears[-1](x))
        
        z = mu + sigma*self.N.sample(mu.shape)
        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()
        return z

class Decoder(nn.Module):
    def __init__(self, hidden_dims = []): 
        super(Decoder, self).__init__()

        self.hidden_dims = hidden_dims[-1::-1]
        self.linears = nn.ModuleList()
        
        for k in range(len(self.hidden_dims) - 1):
            self.linears.append(nn.Linear(self.hidden_dims[k], self.hidden_dims[k + 1]))

    def forward(self, z):
        for linear in self.linears[:-1]:
            # z = F.tanh(linear(z))
            z = F.relu(linear(z))
        z = self.linears[-1](z)

        return z 

class VariationalAutoencoder(nn.Module):
    def __init__(self, full_df, latent_dims, hidden_dims = [], path_nf = ""): 
        super(VariationalAutoencoder, self).__init__()
        
        self.latent_dims = latent_dims
        self.hidden_dims = hidden_dims.copy()
        self.hidden_dims.append(latent_dims)
        # path_nf is the path to the trained NFs for the spatial coordinates of the dataset
        # the model can load the NFs to be able to transform geo coordinates in the latent space, and vice versa
        self.path_nf = path_nf


        self.full_df = full_df.copy()
        self.df = full_df.drop(columns = [u for u in full_df.columns if u in ["x", "y", "x_norm", "y_norm"]]).copy()
        
        df_norm = np.array(self.df)
        self.losses = []
        self.losses1 = []
        self.losses2 = []
        self.losses_geo = []

        self.data = torch.tensor(df_norm).to(torch.float32)
        self.encoder = VariationalEncoder(self.hidden_dims)
        self.decoder = Decoder(self.hidden_dims)
        
        self.df_norm = pd.DataFrame(df_norm, columns = self.df.columns)


    def forward(self, x):
        z = self.encoder(x)
        return self.decoder(z)
    
    def train(self, epochs, lr = 0.001, verbose = False, hide_tqdm = False, 
              batch_size = 100, weight_reconstruction = 1, weight_kl = 1, weights_geo = 0, kl_annealing = True,
              opt_name = "Adam", save_checkpoints = 99999999, checkpoint_path = None):
        if opt_name == "Adam":
            opt = torch.optim.Adam(self.parameters(), lr = lr)
        elif opt_name == "SGD":
            opt = torch.optim.SGD(self.parameters(), lr = lr)
        elif opt_name == "RMSprop":
            opt = torch.optim.RMSprop(self.parameters(), lr = lr)
        
        coords_columns = [j for j,u in enumerate(self.df_norm.columns) if u in ["x_latent", "y_latent"]]

        for epoch in tqdm(range(epochs), disable = hide_tqdm):
            loss_epoch, loss1_epoch, loss2_epoch, loss_geo_epoch = 0,0,0,0
            
            if ((epoch % save_checkpoints) == 0)&(epoch > 0):
                self.save(checkpoint_path)
                
            
            dataloader = DataLoader(self.data, batch_size = batch_size, shuffle = True)
            for batch in dataloader:
                opt.zero_grad()
                # transform the batch of data with encoder-decoder
                x_hat = self(batch)
                
                # reconstruction loss = similarity to data
                loss1 = weight_reconstruction * nn.MSELoss()(batch, x_hat)
                # kl divergence = similarity to normal
                # the kl divergence has been saved by the encoder
                # kl annealing can be used to increase the weight of KL-divergence during the training (first, train to generate realistic data, later, learn to generate different samples)
                loss2 = (weight_kl * epoch / epochs * kl_annealing + weight_kl * (1 - kl_annealing)) * self.encoder.kl / batch_size
                # reconstruciton loss only for geo coordinates
                loss_geo = weights_geo * nn.MSELoss()(batch[:,coords_columns], x_hat[:,coords_columns])
                loss = loss1 + loss2 + loss_geo
            
                loss1_epoch += loss1.item()
                loss2_epoch += loss2.item()
                loss_geo_epoch += loss_geo.item()
                loss.backward()
                opt.step()

            self.losses.append(loss1_epoch + loss2_epoch + loss_geo_epoch)
            self.losses1.append(loss1_epoch)
            self.losses2.append(loss2_epoch)
            self.losses_geo.append(loss_geo_epoch)
            if verbose:
                print(f"{epoch} / {epochs}", round(loss_epoch,1))

    # generate new samples of the trained model
    def get_sample_from_vae(self, nf_dict = None, n_samples = 10000, data = None, seed = None):
        # you can input collected data to transform them
        if data is not None:
            encoded_x = self.encoder(self.data)
        # or you can generate new observation, by sampling from a Normal distribution, and tranforming the samples
        else:
            if seed:
                torch.manual_seed(seed)
            encoded_x = torch.distributions.Normal(loc = torch.zeros(self.latent_dims), 
                                                   scale = torch.ones(self.latent_dims)).sample(torch.tensor([n_samples]))
        decoded_x = self.decoder(encoded_x)
        df_sample = pd.DataFrame(decoded_x.detach().numpy(), columns = self.df.columns)
        #  apply max for categorical features that are stored with one-hor encoding
        df_sample[[u for u in df_sample.columns if "ANNO_COSTRUZIONE" in u]] = df_sample[[u for u in df_sample.columns if "ANNO_COSTRUZIONE" in u]].apply(lambda x: x == x.max(), axis = 1)
        df_sample[[u for u in df_sample.columns if "energy" in u]] = df_sample[[u for u in df_sample.columns if "energy" in u]].apply(lambda x: x == x.max(), axis = 1)
        df_sample[[u for u in df_sample.columns if "COD_CAT" in u]] = df_sample[[u for u in df_sample.columns if "COD_CAT" in u]].apply(lambda x: x == x.max(), axis = 1)
        df_sample[[u for u in df_sample.columns if u[:6] == "floor_"]] = df_sample[[u for u in df_sample.columns if u[:6] == "floor_"]].apply(lambda x: x == x.max(), axis = 1)

        df_sample[[u for u in df_sample.columns if u[:9] == "roomtype_"]] = df_sample[[u for u in df_sample.columns if u[:9] == "roomtype_"]].apply(lambda x: x == x.max(), axis = 1)

        # transform boolean variables in bool, keeping a distribution similar to data
        for var in [u for u in df_sample.columns if "flag" in u]:
            df_sample[var] = df_sample[var] > df_sample[var].quantile(1 - self.df[var].astype(np.float32).mean())

        # transform latent spatial coordinates into realistic normalized coordinates (bounded in [-1,1])
        if nf_dict:
            transf_xy = nfg.nf_latent_to_real(nf_dict, np.array(df_sample[["y_latent", "x_latent"]]))
            df_sample["x_norm"] = transf_xy[:,0]
            df_sample["y_norm"] = transf_xy[:,1]
            
        else:
            df_sample[["x_norm", "y_norm"]] = df_sample[["x_latent", "y_latent"]] * 2 - 1
        df_sample = df_sample.query("(0 < x_latent < 1)&(0 < y_latent < 1)")

        # transform the normalized coordinates into the real ones 
        scaler = MinMaxScaler((-1,1))
        scaler.fit(np.array(self.full_df[["x", "y"]]))
        df_sample[["x", "y"]] = scaler.inverse_transform(np.array(df_sample[["x_norm", "y_norm"]]))

        return df_sample

    def load_nf(self):
        # load the Normalizing Flows, if the path is stored in the correct way
        num_layers, hidden_features, num_epochs, lr_, opt, flow_name, batch_dim, _, date =  self.path_nf.split("/")[-1].split(".")[-2].replace("NSF_CL", "NSF-CL").split("_")
        num_layers, hidden_features, num_epochs = int(num_layers), int(hidden_features), int(num_epochs)
        flow_name = flow_name.replace("NSF-CL", "NSF_CL")
        nf_dict = nfg.load_nf(self.path_nf,  flow_name = flow_name, hidden_features = hidden_features, num_layers = num_layers)
        return nf_dict

    # save the model parameters
    # to load the model we initialize the model, and later we load the saved parameters
    def save(self, path):
        torch.save(self.state_dict(), path)

    # save model settings
    def save_settings(self, path_settings):
        dict_settings = {"hidden_dims": self.hidden_dims,
                         "latent_dims": self.latent_dims,
                         "full_df": self.full_df,
                         "path_nf": self.path_nf
                         }
        with open(path_settings, "wb") as handle:
            pickle.dump(dict_settings, handle)


# to load the trained model, we initialize the model with the correct number of layers, parameters and dimensions
# and later we load the parameters of the trained model 
# (typical routine with torch)
def load_vae(path_vae, hidden_dims_vae, latent_dims_vae, path_nf, df):

    
    vae_ = VariationalAutoencoder(full_df = df,
                                 latent_dims = latent_dims_vae,
                                 hidden_dims = hidden_dims_vae[:-1],
                                 path_nf = path_nf)
    vae_.load_state_dict(torch.load(path_vae))
    
    return vae_