# ************    VAE for generating synthetic populations      ************
# In this script we define and train a model that takes as input a dataframe of real houses, with geographical coordinates and a set of numeric and categorical features
# and learn an encoder and decoder to reproduce a similar population.
# 
# 
# The class VariationalAutoencoder is composed of the two objects, VariationalEncoder and VariationalDecoder.
# The data in full_df are encoded in the latent space, by applying a NN with the layers defined by hidden_dims.
# In order to simplify the generation of realistic geographical coordinates, we transform the real coordinates into a uniform latent space.
# In this way, the VAE need only to learn the mapping from the latent geographical coordinates, while all the spatial constaints are taken into 
# account by the coordinates transformation. 
# The transformation employed is a Normalizing Flows (defined in jl_nflows_geo_coordinates2.py).
# 
# The loss of the VAE is composed by two elements: (i) the reconstruction loss and (ii) the KL-divergence of the encoded data and a Normal distribution 
# (i) measures the distance between the samples generated by the VAE, so it measures how realistic are the samples
# (ii) measures the distance between the encoded space and a Normal distribution, and it controls the overfitting
# In our scenario, we allow for overfitting, as we want to generate samples that can be very close to the collected data.
# (For instance, in image generation, it is typically desired to generate samples different from the collected data)
# Moreover, as the generation of realistic spatial coordinates is the more challenging step, we also allow for stronger overfitting in the geo coordinates.
# For this reason, we set the loss in order to give more weight to reconstruction loss for the geo coordinates, then to reconstruction loss in general, and lower weight to KL-div
# 
# See https://avandekleut.github.io/vae/ as reference
# 
# 
# 
# 





import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils
import torch.distributions
from tqdm import tqdm
from torch.utils.data import DataLoader
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import pandas as pd
import numpy as np
from tqdm import tqdm
import sys
sys.path += ["../src"]
import jl_nflows_geo_coordinates_2 as nfg
from time import time
import pickle
import geopandas as gpd
import utils
import config
from glob import glob

path_pop_synth = "/data/housing/data/intermediate/jl_pop_synth/"
path_intermediate = "/data/housing/data/intermediate/"
cols = ['flag_garage', 'flag_pertinenza',
       'flag_air_conditioning', 'flag_multi_floor',
       'flag_geo_valid', 
    #    'GEO_LATITUDINE_BENE_ROUNDED', 'GEO_LONGITUDINE_BENE_ROUNDED',
    "y", "x",
       'log_mq', 'ANNO_COSTRUZIONE_1500_1965',
       'ANNO_COSTRUZIONE_1965_1985', 'ANNO_COSTRUZIONE_1985_2005',
       'ANNO_COSTRUZIONE_2005_2025', 'ANNO_COSTRUZIONE_Missing',
       'High_energy_class', 'Low_energy_class', 'Medium_energy_class',
       'Missing_energy_class', 'COD_CAT_A02', 'COD_CAT_A03',
       'COD_CAT_A_01_07_08', 'COD_CAT_A_04_05', 'floor_0.0', 'floor_1.0',
       'floor_2.0', 'floor_3.0', 'floor_Missing', 'floor_plus_4', 'prov_abbrv']


class VariationalEncoder(nn.Module):
    def __init__(self, hidden_dims = []):
        super(VariationalEncoder, self).__init__()
        self.hidden_dims = hidden_dims
        self.linears = nn.ModuleList()
        
        for k in range(len(self.hidden_dims) - 1):
            self.linears.append(nn.Linear(self.hidden_dims[k], self.hidden_dims[k + 1]))
        #add one dims for sigma
        self.linears.append(nn.Linear(self.hidden_dims[k], self.hidden_dims[k + 1]))
        

        self.N = torch.distributions.Normal(0, 1)
        self.kl = 0

    def forward(self, x):
        for linear in self.linears[:-2]:
            # x = F.tanh(linear(x))
            x = F.relu(linear(x))
        mu =  self.linears[-2](x)
        sigma = torch.exp(self.linears[-1](x))
        
        z = mu + sigma*self.N.sample(mu.shape)
        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()
        return z

class Decoder(nn.Module):
    def __init__(self, hidden_dims = []): 
        super(Decoder, self).__init__()

        self.hidden_dims = hidden_dims[-1::-1]
        self.linears = nn.ModuleList()
        
        for k in range(len(self.hidden_dims) - 1):
            self.linears.append(nn.Linear(self.hidden_dims[k], self.hidden_dims[k + 1]))

    def forward(self, z):
        for linear in self.linears[:-1]:
            # z = F.tanh(linear(z))
            z = F.relu(linear(z))
        z = self.linears[-1](z)

        return z 

class VariationalAutoencoder(nn.Module):
    def __init__(self, full_df, latent_dims, hidden_dims = [], path_nf = ""): 
        super(VariationalAutoencoder, self).__init__()
        
        self.latent_dims = latent_dims
        self.hidden_dims = hidden_dims.copy()
        self.hidden_dims.append(latent_dims)
        # path_nf is the path to the trained NFs for the spatial coordinates of the dataset
        # the model can load the NFs to be able to transform geo coordinates in the latent space, and vice versa
        self.path_nf = path_nf


        self.full_df = full_df.copy()
        self.df = full_df.drop(columns = [u for u in full_df.columns if u in ["x", "y", "x_norm", "y_norm"]]).copy()
        # scaler = MinMaxScaler((-1,1))
        # scaler.fit(df)
        # df_norm = scaler.transform(df)
        df_norm = np.array(self.df)
        self.losses = []
        self.losses1 = []
        self.losses2 = []
        self.losses_geo = []

        self.data = torch.tensor(df_norm).to(torch.float32)
        self.encoder = VariationalEncoder(self.hidden_dims)
        self.decoder = Decoder(self.hidden_dims)
        
        self.df_norm = pd.DataFrame(df_norm, columns = self.df.columns)


    def forward(self, x):
        z = self.encoder(x)
        return self.decoder(z)
    
    def train(self, epochs, lr = 0.001, verbose = False, hide_tqdm = False, 
              batch_size = 100, weight_reconstruction = 1, weight_kl = 1, weights_geo = 0, kl_annealing = True,
              opt_name = "Adam", save_checkpoints = 99999999, checkpoint_path = None):
        if opt_name == "Adam":
            opt = torch.optim.Adam(self.parameters(), lr = lr)
        elif opt_name == "SGD":
            opt = torch.optim.SGD(self.parameters(), lr = lr)
        elif opt_name == "RMSprop":
            opt = torch.optim.RMSprop(self.parameters(), lr = lr)
        
        coords_columns = [j for j,u in enumerate(self.df_norm.columns) if u in ["x_latent", "y_latent"]]

        for epoch in tqdm(range(epochs), disable = hide_tqdm):
            loss_epoch, loss1_epoch, loss2_epoch, loss_geo_epoch = 0,0,0,0
            
            if ((epoch % save_checkpoints) == 0)&(epoch > 0):
                self.save(checkpoint_path)
                
            
            dataloader = DataLoader(self.data, batch_size = batch_size, shuffle = True)
            for batch in dataloader:
                opt.zero_grad()
                # transform the batch of data with encoder-decoder
                x_hat = self(batch)
                
                # reconstruction loss = similarity to data
                loss1 = weight_reconstruction * nn.MSELoss()(batch, x_hat)
                # kl divergence = similarity to normal
                # the kl divergence has been saved by the encoder
                # kl annealing can be used to increase the weight of KL-divergence during the training (first, train to generate realistic data, later, learn to generate different samples)
                loss2 = (weight_kl * epoch / epochs * kl_annealing + weight_kl * (1 - kl_annealing)) * self.encoder.kl / batch_size
                # reconstruciton loss only for geo coordinates
                loss_geo = weights_geo * nn.MSELoss()(batch[:,coords_columns], x_hat[:,coords_columns])
                loss = loss1 + loss2 + loss_geo
            
                loss1_epoch += loss1.item()
                loss2_epoch += loss2.item()
                loss_geo_epoch += loss_geo.item()
                loss.backward()
                opt.step()

            self.losses.append(loss1_epoch + loss2_epoch + loss_geo_epoch)
            self.losses1.append(loss1_epoch)
            self.losses2.append(loss2_epoch)
            self.losses_geo.append(loss_geo_epoch)
            if verbose:
                print(f"{epoch} / {epochs}", round(loss_epoch,1))

    # generate new samples of the trained model
    def get_sample_from_vae(self, nf_dict = None, n_samples = 10000, data = None, seed = None):
        # you can input collected data to transform them
        if data is not None:
            encoded_x = self.encoder(self.data)
        # or you can generate new observation, by sampling from a Normal distribution, and tranforming the samples
        else:
            if seed:
                torch.manual_seed(seed)
            encoded_x = torch.distributions.Normal(loc = torch.zeros(self.latent_dims), 
                                                   scale = torch.ones(self.latent_dims)).sample(torch.tensor([n_samples]))
        decoded_x = self.decoder(encoded_x)
        df_sample = pd.DataFrame(decoded_x.detach().numpy(), columns = self.df.columns)
        #  apply max for categorical features that are stored with one-hor encoding
        df_sample[[u for u in df_sample.columns if "ANNO_COSTRUZIONE" in u]] = df_sample[[u for u in df_sample.columns if "ANNO_COSTRUZIONE" in u]].apply(lambda x: x == x.max(), axis = 1)
        df_sample[[u for u in df_sample.columns if "energy" in u]] = df_sample[[u for u in df_sample.columns if "energy" in u]].apply(lambda x: x == x.max(), axis = 1)
        df_sample[[u for u in df_sample.columns if "COD_CAT" in u]] = df_sample[[u for u in df_sample.columns if "COD_CAT" in u]].apply(lambda x: x == x.max(), axis = 1)
        df_sample[[u for u in df_sample.columns if u[:6] == "floor_"]] = df_sample[[u for u in df_sample.columns if u[:6] == "floor_"]].apply(lambda x: x == x.max(), axis = 1)

        df_sample[[u for u in df_sample.columns if u[:9] == "roomtype_"]] = df_sample[[u for u in df_sample.columns if u[:9] == "roomtype_"]].apply(lambda x: x == x.max(), axis = 1)

        # transform boolean variables in bool, keeping a distribution similar to data
        for var in [u for u in df_sample.columns if "flag" in u]:
            df_sample[var] = df_sample[var] > df_sample[var].quantile(1 - self.df[var].astype(np.float32).mean())

        # transform latent spatial coordinates into realistic normalized coordinates (bounded in [-1,1])
        if nf_dict:
            transf_xy = nfg.nf_latent_to_real(nf_dict, np.array(df_sample[["y_latent", "x_latent"]]))
            df_sample["x_norm"] = transf_xy[:,0]
            df_sample["y_norm"] = transf_xy[:,1]
            
        else:
            df_sample[["x_norm", "y_norm"]] = df_sample[["x_latent", "y_latent"]] * 2 - 1
        df_sample = df_sample.query("(0 < x_latent < 1)&(0 < y_latent < 1)")

        # transform the normalized coordinates into the real ones 
        scaler = MinMaxScaler((-1,1))
        scaler.fit(np.array(self.full_df[["x", "y"]]))
        df_sample[["x", "y"]] = scaler.inverse_transform(np.array(df_sample[["x_norm", "y_norm"]]))

        return df_sample

    def load_nf(self):
        # load the Normalizing Flows, if the path is stored in the correct way
        num_layers, hidden_features, num_epochs, lr_, opt, flow_name, batch_dim, _, date =  self.path_nf.split("/")[-1].split(".")[-2].replace("NSF_CL", "NSF-CL").split("_")
        num_layers, hidden_features, num_epochs = int(num_layers), int(hidden_features), int(num_epochs)
        flow_name = flow_name.replace("NSF-CL", "NSF_CL")
        nf_dict = nfg.load_nf(self.path_nf,  flow_name = flow_name, hidden_features = hidden_features, num_layers = num_layers)
        return nf_dict

    # save the model parameters
    # to load the model we initialize the model, and later we load the saved parameters
    def save(self, path):
        torch.save(self.state_dict(), path)

    # save model settings
    def save_settings(self, path_settings):
        dict_settings = {"hidden_dims": self.hidden_dims,
                         "latent_dims": self.latent_dims,
                         "full_df": self.full_df,
                         "path_nf": self.path_nf
                         }
        with open(path_settings, "wb") as handle:
            pickle.dump(dict_settings, handle)

# load the trained model, by only specifying model settings and correct paths
def load_vae_province(prov, path_settings, path_vae,
                      abm_path = "ISP_data_for_ABM/ISP_ABM_up_to_2024_08_0001.csv",
                      dropna = True):
    with open(path_settings, "rb") as handle:
        dict_settings = pickle.load(handle)
    vae = load_vae(prov, path_vae, 
                   dict_settings["hidden_dims"], dict_settings["latent_dims"],
                   dropna = dropna,
                   df_vae = dict_settings["full_df"],
                   path_nf = dict_settings["path_nf"],
                   abm_path = abm_path)
    
    return vae


# to load the trained model, we initialize the model with the correct number of layers, parameters and dimensions
# and later we load the parameters of the trained model 
# (typical routine with torch)
def load_vae(prov, path_vae, hidden_dims_vae, latent_dims_vae, path_nf, df_vae = None,
             dropna = True,
             abm_path = "ISP_data_for_ABM/ISP_ABM_up_to_2024_08_0001.csv"):

    if df_vae is None:
        df_vae = get_df_vae(prov, dropna = dropna, 
                            best_nf = path_nf, 
                            abm_path = abm_path)
    
    vae = VariationalAutoencoder(full_df = df_vae,
                                 latent_dims = latent_dims_vae,
                                 hidden_dims = hidden_dims_vae[:-1],
                                 path_nf = path_nf)
    vae.load_state_dict(torch.load(path_vae))
    
    return vae


# load the dataframe for the specified province
# - filter the province
# - select the columns we will use in the training (cols, defined above in this script)
# - normalize the geographical coordinates, in [-1,1] to facilitate the training (they are in [-1,1], instead of Standard Scaler, because Normalizing Flows is implemented from a Uniform(-1,1))

def get_df_prov(prov, dropna = True, 
                drop_cols = [],
                add_cols = [],
                abm_path = "ISP_data_for_ABM/ISP_ABM_up_to_2024_08_0001.csv"):
    data = pd.read_csv(path_intermediate + abm_path, index_col = 0)
    data = data.rename(columns = {"GEO_LATITUDINE_BENE_ROUNDED": "y", "GEO_LONGITUDINE_BENE_ROUNDED": "x"})
    df_ = data[cols + add_cols].query("prov_abbrv == @prov").drop(columns = ["prov_abbrv"] +  drop_cols)

    scaler = MinMaxScaler((-1,1))
    scaler.fit(df_[["x", "y"]])
    df_[["x_norm", "y_norm"]] = scaler.transform(df_[["x", "y"]])

    if dropna:
        df_ = df_[df_[[u for u in df_.columns if "Missing" in u]].sum(axis = 1) == 0].replace("Missing", np.nan).dropna()
    
    return df_


# get the dataframe of a province that will be injected in the VAE
# - load data for the province
# - load the NFs
# - map the spatial coordinates in the latent space of the NFs
def get_df_vae(prov, dropna = True, 
               best_nf = "32_32_20000_10_Adam_NSF_CL_100_nf_241115.pkl", 
               abm_path = "ISP_data_for_ABM/ISP_ABM_up_to_2024_08_0001.csv"):

    df_ = get_df_prov(prov = prov, dropna = dropna, abm_path = abm_path)
    path_nf = path_intermediate + "jl_pop_synth/" + best_nf
    num_layers, hidden_features, num_epochs, lr_, opt, flow_name, batch_dim, _, date =  best_nf.replace("NSF_CL", "NSF-CL").split("/")[-1].split("_")
    num_layers, hidden_features, num_epochs = int(num_layers), int(hidden_features), int(num_epochs)
    flow_name = flow_name.replace("NSF-CL", "NSF_CL")
    nf_dict = nfg.load_nf(path_nf,  flow_name = flow_name, hidden_features = hidden_features, num_layers = num_layers)

    scaler = MinMaxScaler(feature_range = (-1,1))
    scaler.fit(df_[["y", "x"]])
    df_vae = df_.copy()
    scaled_coord = scaler.transform(df_[["y", "x"]])
    inv_coord, _ = nf_dict["flow"].flow.forward(torch.tensor(scaled_coord, dtype = torch.float32))
    df_vae[["y_latent", "x_latent"]] = torch.sigmoid(inv_coord[-1]).detach().numpy()

    return df_vae

def get_old_df_vae(bins_construction = 20, bins_energy_classes = 3, drop_COD_CAT_CATASTALE = True, drop_villa = False,
               drop_energy_class = False, drop_mq = False, drop_construction = False, other_drop = []):
    isp_bo = pd.read_csv(path_pop_synth + "isp_bo.csv", index_col = 0)
    df_vae = isp_bo[["OMI_id", "CAP", "log_price", #"IMP_FMP_PREZZO_FIN",
                     "GEO_LATITUDINE_BENE_ROUNDED", "GEO_LONGITUDINE_BENE_ROUNDED",
                     "energy_class","mq_bins", "flag_garage", "flag_pertinenza", "anno_costr_bins",
                     "flag_air_conditioning", "flag_multi_floor", "scenario_Risk", "floor_numeric", 
                     "COD_CAT_CATASTALE", "villa"]]

    if bins_construction == 20:
        df_vae = df_vae.replace({'1500-1955': "pre-1965",
                        '1955-1960': "pre-1965",
                        '1960-1965': "pre-1965", 
                        '1965-1970': "1965-1985",
                        '1970-1975': "1965-1985", 
                        '1975-1985': "1965-1985",
                        '1985-1995': "1985-2005", 
                        '1995-2005': "1985-2005",
                        '2005-2015': "post-2005",
                        '2015-2025': "post-2005"})
    if bins_energy_classes == 3:
        df_vae = df_vae.replace({"A1": "high",
                                "A2": "high",
                                "A3": "high",
                                "A4": "high",
                                "B": "medium",
                                "C": "medium",
                                "D": "medium",
                                "E": "low",
                                "F": "low",
                                "G": "low",
                                })
    # NaN su energy_class_numeric, flag_multi_floor and flag_air_conditioning (Bologna, from 13k to 8k)
    df_vae = df_vae.replace({"Missing": np.nan})
    df_vae = df_vae.dropna()
    if drop_energy_class:
        df_vae = df_vae.drop(columns = ["energy_class"])
    else:
        df_vae = pd.get_dummies(df_vae, columns = ["energy_class"], prefix = "energy_class")
    
    if drop_mq:
        df_vae = df_vae.drop(columns = ["mq_bins"])
    else:
        df_vae = pd.get_dummies(df_vae, columns = ["mq_bins"], prefix = "mq")
    
    if drop_construction:
        df_vae = df_vae.drop(columns = ["anno_costr_bins"])
    else:
        df_vae = pd.get_dummies(df_vae, columns = ["anno_costr_bins"], prefix = "construction")
    
    if drop_COD_CAT_CATASTALE:
        df_vae = df_vae.drop(columns = ["COD_CAT_CATASTALE"])
    else:
        df_vae = pd.get_dummies(df_vae, columns = ["COD_CAT_CATASTALE"], prefix = "CAT")
    
    if drop_villa:
        df_vae = df_vae.drop(columns = ["villa"])
    
    df_vae["ground_floor"] = df_vae["floor_numeric"] == "0.0"
    df_vae = df_vae.drop(columns = other_drop)
    
    df_vae = df_vae.drop(columns = ["OMI_id", "CAP", "scenario_Risk", "floor_numeric"]).astype(np.float32)
    df_vae = df_vae.drop(columns = ["log_price"])
    
    return df_vae

# load the data that are used in the spatial matching
# spatial matching will associate province, comune, risk, OMI, CAP to the geo coordinates
"""def load_geo_data():
    hydro_risk = gpd.read_file(config.hydro_risk_path).to_crs('EPSG:3035')
    hydro_risk.drop(columns = 'PRO_COM',inplace=True)
    census = gpd.read_file(config.census_hydro_risk_path).to_crs('EPSG:3035') 
    omi_og = gpd.read_file(config.omi_shapefile_path)
    omi_og = omi_og.to_crs('EPSG:3035')
    cap = gpd.read_file(config.cap_shapefile_path)
    cap = cap.to_crs('EPSG:3035')
    return {"hydro_risk": hydro_risk, "census": census, "omi_og": omi_og, "cap": cap}
"""

def load_geo_data():
    with open("/data/housing/data/intermediate/jl_pop_synth/geo_dict.pkl", "rb") as f:
        geo_dict = pickle.load(f)
    return geo_dict
    

# run the script for training and saving a VAE for a province
# with the desired implementation and optimization of the VAE
if __name__ == "__main__":
    prov, latent_dims_vae, flows_conf_vae, num_epochs_vae, lr_vae_, opt_vae, date =  sys.argv[1:]
    
    df_vae = get_df_vae(prov, dropna = True, 
                        best_nf = "32_32_20000_10_Adam_NSF_CL_100_nf_241115.pkl", 
                        abm_path = "ISP_data_for_ABM/ISP_ABM_up_to_2024_08_0001.csv")
    init_dims = df_vae.drop(columns = ["x", "y"]).shape(1)
    print(int(flows_conf_vae))
    # flows_conf_vae chooses for the configuration of layers, among these ones
    hidden_dims_vae = [[init_dims],[init_dims, 16],[init_dims, 20, 16],[init_dims, 20, 20, 16]][int(flows_conf_vae)]
    lr_vae = int(lr_vae_) / 1000000
    # lr_nf, lr_vae = lr_nf_ / 10000, lr_vae_ / 10000
    print(latent_dims_vae, flows_conf_vae, lr_vae, opt_vae)

    vae = VariationalAutoencoder(df = df_vae.drop(columns = ["x", "y"]), 
                                 latent_dims = int(latent_dims_vae), 
                                 hidden_dims = hidden_dims_vae)
    vae.train(epochs = int(num_epochs_vae), 
              lr = lr_vae, hide_tqdm = True, 
              verbose = False, opt_name = opt_vae)
    
    id = "_".join([latent_dims_vae, flows_conf_vae, num_epochs_vae, lr_vae_, opt_vae, "vae", date])
    torch.save(vae.state_dict(), path_pop_synth + f"{id}.pth")
    